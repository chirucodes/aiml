{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chirucodes/aiml/blob/main/with_attention_imageCaptioningWithInceptionv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziGypaDO078C"
      },
      "outputs": [],
      "source": [
        "!rm -rf Flickr8k_Dataset.zip\n",
        "!rm -rf Flickr8k_text.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip -O Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip -O Flickr8k_text.zip\n",
        "!unzip Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_text.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ9Uc8oB093H"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# load the file containing all of the captions into a single long string\n",
        "#--------------------------------------------------\n",
        "caption_file = \"Flickr8k.token.txt\"\n",
        "def load_captions (filename):\n",
        "  with open(filename, \"r\") as fp:\n",
        "    # Read all text in the file\n",
        "    text = fp.read()\n",
        "    return (text)\n",
        "\n",
        "#--------------------------------------------------\n",
        "# Each photo has a unique identifier, which is the file name of the image .jpg file\n",
        "# Create a dictionary of photo identifiers (without the .jpg) to captions. Each photo identifier maps to\n",
        "# a list of one or more textual descriptions.\n",
        "#\n",
        "# {\"image_name_1\" : [\"caption 1\", \"caption 2\", \"caption 3\"],\n",
        "#  \"image_name_2\" : [\"caption 4\", \"caption 5\"]}\n",
        "#--------------------------------------------------\n",
        "def captions_dict (text):\n",
        "  dict = {}\n",
        "  \n",
        "  # Make a List of each line in the file\n",
        "  lines = text.split ('\\n')\n",
        "  for line in lines:\n",
        "    \n",
        "    # Split into the <image_data> and <caption>\n",
        "    line_split = line.split ('\\t')\n",
        "    if (len(line_split) != 2):\n",
        "      # Added this check because dataset contains some blank lines\n",
        "      continue\n",
        "    else:\n",
        "      image_data, caption = line_split\n",
        "\n",
        "    # Split into <image_file> and <caption_idx>\n",
        "    image_file, caption_idx = image_data.split ('#')\n",
        "    # Split the <image_file> into <image_name>.jpg\n",
        "    image_name = image_file.split ('.')[0]\n",
        "    \n",
        "    # If this is the first caption for this image, create a new list for that\n",
        "    # image and add the caption to it. Otherwise append the caption to the \n",
        "    # existing list\n",
        "    if (int(caption_idx) == 0):\n",
        "      dict [image_name] = [caption]\n",
        "    else:\n",
        "      dict [image_name].append (caption)\n",
        "  \n",
        "  return (dict)\n",
        "\n",
        "doc = load_captions (caption_file)\n",
        "image_dict = captions_dict (doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suK-erKC1NV8"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# We have three separate files which contain the names for the subset of \n",
        "# images to be used for training, validation or testing respectively\n",
        "#\n",
        "# Given a file, we return a set of image names (without .jpg extension) in that file\n",
        "#--------------------------------------------------\n",
        "def subset_image_name (filename):\n",
        "  data = []\n",
        "  \n",
        "  with open(filename, \"r\") as fp:\n",
        "    # Read all text in the file\n",
        "    text = fp.read()\n",
        "  \n",
        "    # Make a List of each line in the file\n",
        "    lines = text.split ('\\n')\n",
        "    for line in lines:\n",
        "      # skip empty lines\n",
        "      if (len(line) < 1):\n",
        "        continue\n",
        "      \n",
        "      # Each line is the <image_file>\n",
        "      # Split the <image_file> into <image_name>.jpg\n",
        "      image_name = line.split ('.')[0]\n",
        "      \n",
        "      # Add the <image_name> to the list\n",
        "      data.append (image_name)\n",
        "\n",
        "    return (set(data))  \n",
        "\n",
        "training_image_name_file = \"Flickr_8k.trainImages.txt\"\n",
        "training_image_names = subset_image_name (training_image_name_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCFYPWNa1RX2",
        "outputId": "28226fcd-6e0c-43c5-db6a-95772bd7cb95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 375/375 [26:21<00:00,  4.22s/it]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path\n",
        "    \n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "image_dir = \"Flicker8k_Dataset/\"\n",
        "training_image_paths = [image_dir + name + '.jpg' for name in training_image_names]\n",
        "\n",
        "# Get unique images\n",
        "encode_train = sorted(set(training_image_paths))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in tqdm(image_dataset):\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgTx1h-x1WFe"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "#--------------------------------------------------\n",
        "# Clean the captions data\n",
        "#    Convert all words to lowercase.\n",
        "#    Remove all punctuation.\n",
        "#    Remove all words that are one character or less in length (e.g. ‘a’).\n",
        "#    Remove all words with numbers in them.\n",
        "#--------------------------------------------------\n",
        "def captions_clean (image_dict):\n",
        "  # <key> is the image_name, which can be ignored\n",
        "  for key, captions in image_dict.items():\n",
        "    \n",
        "    # Loop through each caption for this image\n",
        "    for i, caption in enumerate (captions):\n",
        "      \n",
        "      # Convert the caption to lowercase, and then remove all special characters from it\n",
        "      caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
        "      \n",
        "      # Split the caption into separate words, and collect all words which are more than \n",
        "      # one character and which contain only alphabets (ie. discard words with mixed alpha-numerics)\n",
        "      clean_words = [word for word in caption_nopunct.split() if ((len(word) > 1) and (word.isalpha()))]\n",
        "      \n",
        "      # Join those words into a string\n",
        "      caption_new = ' '.join(clean_words)\n",
        "      \n",
        "      # Replace the old caption in the captions list with this new cleaned caption\n",
        "      captions[i] = caption_new\n",
        "      \n",
        "#--------------------------------------------------\n",
        "# Add two tokens, 'startseq' and 'endseq' at the beginning and end respectively, \n",
        "# of every caption\n",
        "#--------------------------------------------------\n",
        "def add_token (captions):\n",
        "  for i, caption in enumerate (captions):\n",
        "    captions[i] = 'startseq ' + caption + ' endseq'\n",
        "  return (captions)\n",
        "\n",
        "#--------------------------------------------------\n",
        "# Given a set of training, validation or testing image names, return a dictionary\n",
        "# containing the corresponding subset from the full dictionary of images with captions\n",
        "#\n",
        "# This returned subset has the same structure as the full dictionary\n",
        "# {\"image_name_1\" : [\"caption 1\", \"caption 2\", \"caption 3\"],\n",
        "#  \"image_name_2\" : [\"caption 4\", \"caption 5\"]}\n",
        "#--------------------------------------------------\n",
        "def subset_data_dict (image_dict, image_names):\n",
        "  dict = { image_name:add_token(captions) for image_name,captions in image_dict.items() if image_name in image_names}\n",
        "  return (dict)\n",
        "\n",
        "#--------------------------------------------------\n",
        "# Flat list of all captions\n",
        "#--------------------------------------------------\n",
        "def all_captions (data_dict):\n",
        "  return ([caption for key, captions in data_dict.items() for caption in captions])\n",
        "\n",
        "#--------------------------------------------------\n",
        "# Calculate the word-length of the caption with the most words\n",
        "#--------------------------------------------------\n",
        "def max_caption_length(captions):\n",
        "  return max(len(caption.split()) for caption in captions)\n",
        "\n",
        "#--------------------------------------------------\n",
        "# Fit a Keras tokenizer given caption descriptions\n",
        "# The tokenizer uses the captions to learn a mapping from words to numeric word indices\n",
        "#\n",
        "# Later, this tokenizer will be used to encode the captions as numbers\n",
        "#--------------------------------------------------\n",
        "def create_tokenizer(data_dict):\n",
        "  captions = all_captions(data_dict)\n",
        "  max_caption_words = max_caption_length(captions)\n",
        "  \n",
        "  # Initialise a Keras Tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "  \n",
        "  # Fit it on the captions so that it prepares a vocabulary of all words\n",
        "  tokenizer.fit_on_texts(captions)\n",
        "  \n",
        "  # Get the size of the vocabulary\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  return (tokenizer, vocab_size, max_caption_words)\n",
        "\n",
        "#--------------------------------------------------\n",
        "# Extend a list of text indices to a given fixed length\n",
        "#--------------------------------------------------\n",
        "def pad_text (text, max_length): \n",
        "  text = pad_sequences([text], maxlen=max_length, padding='post')[0]\n",
        "  \n",
        "  return (text)\n",
        "\n",
        "captions_clean (image_dict)\n",
        "training_dict = subset_data_dict (image_dict, training_image_names)\n",
        "\n",
        "# Prepare tokenizer\n",
        "tokenizer, vocab_size, max_caption_words = create_tokenizer(training_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Guq8O1BCAMH"
      },
      "outputs": [],
      "source": [
        "print(tokenizer)\n",
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5boHdD_F1ZoN"
      },
      "outputs": [],
      "source": [
        "def data_prep(data_dict, tokenizer, max_length, vocab_size):\n",
        "  X, y = list(), list()\n",
        "\n",
        "  # For each image and list of captions\n",
        "  for image_name, captions in data_dict.items():\n",
        "    image_name = image_dir + image_name + '.jpg'\n",
        "\n",
        "    # For each caption in the list of captions\n",
        "    for caption in captions:\n",
        "\n",
        "      # Convert the caption words into a list of word indices\n",
        "      word_idxs = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "      # Pad the input text to the same fixed length\n",
        "      pad_idxs = pad_text(word_idxs, max_length)\n",
        "          \n",
        "      X.append(image_name)\n",
        "      y.append(pad_idxs)\n",
        "  \n",
        "  #return array(X), array(y)\n",
        "  return X, y\n",
        "\n",
        "train_X, train_y = data_prep(training_dict, tokenizer, max_caption_words, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTldUDWt1dQ8"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "   img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "   return img_tensor, cap\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjWPFm2y1iPy"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTrWpJT5BGhB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "uQHpzssN1jd3",
        "outputId": "3db5fe4d-483e-4877-ddc9-6bd637bfe7b6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c3bd9828746d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Shape of the vector extracted from InceptionV3 is (64, 2048)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ],
      "source": [
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = vocab_size\n",
        "num_steps = len(train_X) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "loss_plot = []\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['startseq']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss\n",
        "\n",
        "import time\n",
        "start_epoch = 0\n",
        "EPOCHS = 1\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCaNApxX1xVm"
      },
      "outputs": [],
      "source": [
        "def evaluate(image, max_length):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['startseq']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == 'endseq':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n",
        "  \n",
        "  def check_test(test_image_names, image_dict, image_dir, max_caption_words):\n",
        "  # captions on the validation set\n",
        "  rid = np.random.randint(0, len(test_image_names))\n",
        "  image_name = test_image_names[rid]\n",
        "  real_caption = image_dict[image_name]\n",
        "\n",
        "  image_path = image_dir + image_name + '.jpg'\n",
        "  result, attention_plot = evaluate(image_path, max_caption_words)\n",
        "\n",
        "  #from IPython.display import Image, display\n",
        "  #display(Image(image_path))\n",
        "  print('Real Caption:', real_caption)\n",
        "  print('Prediction Caption:', ' '.join(result))\n",
        "  \n",
        "test_image_name_file = \"Flickr_8k.testImages.txt\"\n",
        "test_image_names = subset_image_name (test_image_name_file)\n",
        "image_dir = \"dataset/Flicker8k_Dataset/\"\n",
        "check_test(list(test_image_names), image_dict, image_dir, max_caption_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "with_attention_imageCaptioningWithInceptionv3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}